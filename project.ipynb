{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":69160,"sourceType":"modelInstanceVersion","modelInstanceId":57690,"modelId":79606},{"sourceId":69627,"sourceType":"modelInstanceVersion","modelInstanceId":58099,"modelId":79606}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T09:15:17.083062Z","iopub.execute_input":"2025-11-21T09:15:17.083337Z","iopub.status.idle":"2025-11-21T09:15:17.361338Z","shell.execute_reply.started":"2025-11-21T09:15:17.083316Z","shell.execute_reply":"2025-11-21T09:15:17.360606Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/qwen2/transformers/1.5b-instruct/1/config.json\n/kaggle/input/qwen2/transformers/1.5b-instruct/1/merges.txt\n/kaggle/input/qwen2/transformers/1.5b-instruct/1/LICENSE\n/kaggle/input/qwen2/transformers/1.5b-instruct/1/README.md\n/kaggle/input/qwen2/transformers/1.5b-instruct/1/tokenizer.json\n/kaggle/input/qwen2/transformers/1.5b-instruct/1/vocab.json\n/kaggle/input/qwen2/transformers/1.5b-instruct/1/tokenizer_config.json\n/kaggle/input/qwen2/transformers/1.5b-instruct/1/model.safetensors\n/kaggle/input/qwen2/transformers/1.5b-instruct/1/generation_config.json\n/kaggle/input/qwen2/transformers/7b-instruct/1/model.safetensors.index.json\n/kaggle/input/qwen2/transformers/7b-instruct/1/model-00003-of-00004.safetensors\n/kaggle/input/qwen2/transformers/7b-instruct/1/config.json\n/kaggle/input/qwen2/transformers/7b-instruct/1/merges.txt\n/kaggle/input/qwen2/transformers/7b-instruct/1/LICENSE\n/kaggle/input/qwen2/transformers/7b-instruct/1/model-00001-of-00004.safetensors\n/kaggle/input/qwen2/transformers/7b-instruct/1/README.md\n/kaggle/input/qwen2/transformers/7b-instruct/1/tokenizer.json\n/kaggle/input/qwen2/transformers/7b-instruct/1/vocab.json\n/kaggle/input/qwen2/transformers/7b-instruct/1/tokenizer_config.json\n/kaggle/input/qwen2/transformers/7b-instruct/1/model-00004-of-00004.safetensors\n/kaggle/input/qwen2/transformers/7b-instruct/1/model-00002-of-00004.safetensors\n/kaggle/input/qwen2/transformers/7b-instruct/1/generation_config.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nprint(\"GPU0 free:\", torch.cuda.mem_get_info(0)[0] / 1024**3, \"GB\")\nprint(\"GPU1 free:\", torch.cuda.mem_get_info(1)[0] / 1024**3, \"GB\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install --quiet transformers sentence-transformers tqdm accelerate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install --quiet chromadb gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install protobuf==3.20.3 --quiet --force-reinstall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, json, torch, time\nfrom datasets import load_dataset\nimport chromadb\nfrom chromadb.utils import embedding_functions\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport gradio as gr\nfrom typing import List, Tuple, Dict\n\n# Paths\nCHROMA_PATH = \"/kaggle/working/chroma_db\"\nQWEN_PATH = \"/kaggle/working/Qwen2-7B-Instruct\"  # Change if different\nCOLLECTION_NAME = \"health_rag_collection\"\n\n# Models\nEMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Agent settings\nTOPK_INITIAL = 5\nTOPK_EXPANDED = 10\nCONF_THRESHOLD = 0.75\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T11:02:45.763949Z","iopub.execute_input":"2025-11-20T11:02:45.764693Z","iopub.status.idle":"2025-11-20T11:02:45.768976Z","shell.execute_reply.started":"2025-11-20T11:02:45.764665Z","shell.execute_reply":"2025-11-20T11:02:45.768196Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Core datasets\nds_health = load_dataset(\"katielink/healthsearchqa\")       # Layman queries\nds_mediqa = load_dataset(\"bigbio/mediqa_qa\")               # Patient-oriented answers\nds_medquad = load_dataset(\"lavita/MedQuAD\")                # Medical grounding\n\nprint(\"Datasets Loaded:\")\nprint(\"HealthSearchQA:\", len(ds_health['train']))\nprint(\"MediQA-QA:\", len(ds_mediqa['train']))\nprint(\"MedQuAD:\", len(ds_medquad['train']))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = chromadb.PersistentClient(path=CHROMA_PATH)\nembed_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)\ncollection = client.get_or_create_collection(name=COLLECTION_NAME, embedding_function=embed_fn)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if collection.count() == 0:\n    docs, metas, ids = [], [], []\n    for row in ds_medquad['train'][:3000]:  # limit for Kaggle\n        text = row.get('answer', '') or row.get('description','')\n        docs.append(text)\n        metas.append({\"topic\": row.get('disease', ''), \"source\": \"MedQuAD\"})\n        ids.append(str(len(ids)))\n    collection.add(documents=docs, metadatas=metas, ids=ids)\n    print(\"Ingested to Chroma:\", collection.count(), \"documents\")\nelse:\n    print(\"Chroma index already exists:\", collection.count())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(QWEN_PATH)\nmodel = AutoModelForCausalLM.from_pretrained(QWEN_PATH, torch_dtype=\"auto\", device_map=\"auto\")\nmodel.eval()\n\ndef chat(system, user, max_tokens=512):\n    msgs=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n    text=tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n    inputs=tokenizer(text, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        out=model.generate(**inputs, max_new_tokens=max_tokens, temperature=0.0)\n    return tokenizer.decode(out[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def retrieve(query, k=TOPK_INITIAL):\n    result = collection.query(query_texts=[query], n_results=k)\n    return list(zip(result['documents'][0], result['metadatas'][0]))\n\ndef format_context(docs):\n    return \"\\n\\n\".join([f\"[Doc {i+1}] {d[:400]}\" for i,(d,_) in enumerate(docs)])\n\n# Layman reformulation\ndef reformulate_query(query):\n    sys = \"You convert patient queries into medically precise search terms.\"\n    return chat(sys, f\"Original: {query}\\nReformulate: \")\n\n# Clarify question\ndef generate_clarify(query):\n    sys=\"If query is unclear, ask ONE follow-up question.\"\n    return chat(sys, f\"Patient query: {query}\\nAsk clarification only if needed:\")\n\n# Risk module\ndef risk_awareness(response):\n    sys = \"Provide risk awareness (non-advice) based on this answer\"\n    return chat(sys, f\"Based on: {response}\\nGenerate education-only risk awareness:\")\n\n# Doctor discussion prompt\ndef doctor_questions(response):\n    sys = \"Give 1-2 questions patient should ask doctor (no advice)\"\n    return chat(sys, f\"Response: {response}\\nSuggest questions:\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def agentic_rag(query, mode=\"patient\"):\n    clarify = generate_clarify(query)\n    if \"?\" in clarify:\n        query = clarify\n\n    med_query = reformulate_query(query)\n    docs = retrieve(med_query)\n\n    sys = \"You answer using only context.\"\n    usr = f\"Q: {query}\\nContext:\\n{format_context(docs)}\"\n    answer = chat(sys, usr)\n\n    eval_sys = \"Evaluate response in JSON: {'score':0-1,'needs_retry':bool}\"\n    eval_res = chat(eval_sys, f\"Q:{query}\\nAnswer:{answer}\")\n    try:\n        ev = json.loads(eval_res)\n    except:\n        ev = {\"score\":0.0,\"needs_retry\":True}\n\n    if ev['score'] < CONF_THRESHOLD:\n        docs = retrieve(med_query, TOPK_EXPANDED)\n        answer = chat(sys, f\"Q:{query}\\nContext:\\n{format_context(docs)}\")\n\n    if mode==\"patient\":\n        answer = chat(\"Simplify & support emotionally\", f\"{answer}\\nPatient version:\")\n        answer += \"\\n\\nâš ï¸ \" + risk_awareness(answer)\n        answer += \"\\n\\nðŸ©º Questions to ask your doctor:\\n- \" + doctor_questions(answer)\n\n    return answer, ev\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Test queries\ntest_queries = [\n    \"I have high blood pressure and Iâ€™m going for surgery. Should I worry?\",\n    \"I have diabetes and I'm scheduled for cataract surgery. Is it risky?\",\n    \"I have asthma but I often ignore inhalers. Can it affect anesthesia?\",\n    \"I take multiple heart medications. Should I inform my doctor before treatment?\",\n    \"I often skip sugar check-ups. Will it slow down recovery after surgery?\"\n]\n\n# 2. Evaluate system\nfrom time import time\n\ndef evaluate_system(queries, mode=\"patient\"):\n    results = []\n    for q in queries:\n        start = time()\n        ans, ev = agentic_rag(q, mode)\n        elapsed = time() - start\n        \n        results.append({\n            \"query\": q,\n            \"confidence_score\": ev.get(\"score\", None),\n            \"needed_retry\": ev.get(\"needs_retry\", None),\n            \"response_time_sec\": round(elapsed, 2),\n            \"response_preview\": ans[:150] + \"...\"  # avoid large outputs\n        })\n    return results\n\nresults = evaluate_system(test_queries)\nimport pandas as pd\ndf_results = pd.DataFrame(results)\n\n# 3. Summary metrics\nsummary = {\n    \"avg_confidence\": round(df_results[\"confidence_score\"].mean(), 3),\n    \"retry_rate_%\": round((df_results[\"needed_retry\"].sum() / len(df_results)) * 100, 2),\n    \"avg_response_time_sec\": round(df_results[\"response_time_sec\"].mean(), 2)\n}\n\ndf_results, summary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ui_handler(query, mode):\n    try:\n        ans, evaluation = agentic_rag(query, mode)\n        return ans, json.dumps(evaluation, indent=2)\n    except Exception as e:\n        return f\"Error: {e}\", \"N/A\"\n\nwith gr.Blocks() as ui:\n    gr.Markdown(\"# ðŸ¤– Agentic RAG Healthcare Assistant\\n(MVP Prototype)\")\n    inp = gr.Textbox(label=\"Enter your question\")\n    mode = gr.Radio([\"patient\", \"professional\"], value=\"patient\", label=\"Response Mode\")\n    btn = gr.Button(\"Generate\")\n    out = gr.Markdown()\n    ev = gr.JSON()\n\n    btn.click(ui_handler, inputs=[inp, mode], outputs=[out, ev])\n\nui.launch(share=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}